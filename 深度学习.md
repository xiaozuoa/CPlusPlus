资料来源于：https://www.bilibili.com/video/BV1Wq4y1L7Tu?spm_id_from=333.337.search-card.all.click&vd_source=668a7f79e6be6b34dd1bc75256e0ad32

## 长短期记忆网络LSTM

### 传统的RNN

![image-20220718114703510](C:\Users\zgliang\AppData\Roaming\Typora\typora-user-images\image-20220718114703510.png)

左边是输入，中间是隐层，右边是输出。

计算隐层状态St的时候，是通过输入Xt和上一时刻的隐层状态S t-1 这两项来计算的。

实现了记忆的效果，但是只是基于前一时刻，是一种short memory。

### LSTM

![image-20220718115647737](C:\Users\zgliang\AppData\Roaming\Typora\typora-user-images\image-20220718115647737.png)

和RNN相比，LSTM增加了一条新的时间链，我们用c来表示，并且增加了时间链和中间隐层的关系。

计算隐层状态St的时候，除了输入Xt和上一时刻的隐层状态S t-1 这两项之外，还需要当前时刻的日记信息Ct。



它们之间的关系是这样的：

![image-20220718121143514](C:\Users\zgliang\AppData\Roaming\Typora\typora-user-images\image-20220718121143514.png)

第一步删除旧的：根据Xt和S t-1 来决定修改日记中的哪些记录

第二步增加新的。

Ct的得出方式是：![image-20220718121316960](C:\Users\zgliang\AppData\Roaming\Typora\typora-user-images\image-20220718121316960.png)



![image-20220718121507787](C:\Users\zgliang\AppData\Roaming\Typora\typora-user-images\image-20220718121507787.png)



LSTM比传统RNN多引入了很多参数，所以训练起来要麻烦一些



## 全连接层

### 卷积神经网络之全连接层

在全连接层中所有神经元都有权重连接，通常全连接层在卷积神经网络尾部。当前面卷积层抓取到足以用来识别图片的特征后，接下来的就是如何进行分类。 通常卷积网络的最后会将末端得到的长方体平摊成一个长长的向量，并送入全连接层配合输出层进行分类。比如，在下面图中我们进行的图像分类为四分类问题，所以卷积神经网络的输出层就会有四个神经元。

![img](https://pic4.zhimg.com/80/v2-8184c2c541e8e0ca2bd5cf9b065eb0b7_720w.jpg)

上面是四分类问题



## 熵有关名词解释

https://www.bilibili.com/video/BV15V411W7VB?spm_id_from=333.337.search-card.all.click&vd_source=668a7f79e6be6b34dd1bc75256e0ad32讲的交叉熵公式的由来和推导，十分清晰明了。

### 熵（信息熵、交叉熵、相对熵、交叉熵损失）

#### 熵

熵是一个物理学概念，表示一个系统的不确定性程度，或者说一个系统的混乱程度。

信息熵又叫香农熵。

熵 vs 信息熵  类似于  帅哥 vs 靓仔，其实是一个东西的不同叫法。

信息熵的公式：

![image-20220718151034138](C:\Users\zgliang\AppData\Roaming\Typora\typora-user-images\image-20220718151034138.png)

举例如下：

例子一：

![image-20220718151321993](C:\Users\zgliang\AppData\Roaming\Typora\typora-user-images\image-20220718151321993.png)

上面的图片是以10为底数的。

![image-20220718151819479](C:\Users\zgliang\AppData\Roaming\Typora\typora-user-images\image-20220718151819479.png)

例子二：

![image-20220718152331942](C:\Users\zgliang\AppData\Roaming\Typora\typora-user-images\image-20220718152331942.png)

#### 相对熵（KL散度）

相对熵就是KL散度，KL散度就是两个概率分布间差异的非对称性度量，通俗说法就是：KL散度是用来衡量同一个随机变量的两个不同分布之间的距离。

KL散度公式：

![image-20220718153326489](C:\Users\zgliang\AppData\Roaming\Typora\typora-user-images\image-20220718153326489.png)

![image-20220718153525510](C:\Users\zgliang\AppData\Roaming\Typora\typora-user-images\image-20220718153525510.png)

举例如下：

![image-20220718153726575](C:\Users\zgliang\AppData\Roaming\Typora\typora-user-images\image-20220718153726575.png)

KL散度公式变形：

![image-20220718154333412](C:\Users\zgliang\AppData\Roaming\Typora\typora-user-images\image-20220718154333412.png)

所以，KL散度=交叉熵-信息熵。KL散度的数值（根据吉布斯不等式可以证明KL散度数值绝对不为负数的）越接近0，那么这两个系统越像。所以交叉熵越小，KL散度越接近于0，所以损失函数可以直接用交叉熵来使用，交叉熵越小那么两个概率模型越接近。

如果这个不懂的话，可以往上翻一下熵的公式和下面的交叉熵公式。

#### 交叉熵

交叉熵主要应用：主要度量同一个随机变量X的预测分布W与**真实分布P**之间的差距。（**计算差距，所以常用来作损失函数**）

这个差距可以理解为：距离、误差、失望值、困难程度、混乱程度等。

交叉熵公式如下：

![image-20220718155347500](C:\Users\zgliang\AppData\Roaming\Typora\typora-user-images\image-20220718155347500.png)

举例如下：

![image-20220718155744765](C:\Users\zgliang\AppData\Roaming\Typora\typora-user-images\image-20220718155744765.png)

可以观察到，预测的越准确，交叉熵的值就越小。

在上面的例子中，可以得到以下结论：

![image-20220718160736277](C:\Users\zgliang\AppData\Roaming\Typora\typora-user-images\image-20220718160736277.png)

##### 交叉熵的最简公式：

![image-20220718161023913](C:\Users\zgliang\AppData\Roaming\Typora\typora-user-images\image-20220718161023913.png)

上面公式中的q（Ci）就是预测真实标签的概率，只是把0*的一些数字给隐去了。

##### 交叉熵的二分类公式：

![image-20220718163318661](C:\Users\zgliang\AppData\Roaming\Typora\typora-user-images\image-20220718163318661.png)

通俗的讲，只有两个分类。

下面两个式子意为，概率分布的和是1，预测分布的和也为1。

![image-20220718164603345](C:\Users\zgliang\AppData\Roaming\Typora\typora-user-images\image-20220718164603345.png)



##### 交叉熵应用在神经网络当中

![image-20220719101015304](C:\Users\zgliang\AppData\Roaming\Typora\typora-user-images\image-20220719101015304.png)

注意公式中的m，如果两个系统的发生事件数量是一样的，那么直接用即可。如果 系统Q的数量n > 系统P的数量m，那么公式中的m直接用系统Q的数量n即可（直接用两个系统中更大的数即可）。



#### 思考：为什么模型中，使用交叉熵做损失函数而不使用KL散度做损失函数

![image-20220718164821801](C:\Users\zgliang\AppData\Roaming\Typora\typora-user-images\image-20220718164821801.png)

有真实分布，真实分布的信息熵为0，此时的KL散度等于交叉熵，所以用交叉熵。如果没有真实分布，请用KL散度。本质上都是KL散度（此句为笔者猜测）！

### KL散度、softmax、sigmoid

#### KL散度就是相对熵。



#### softmax

![image-20220718170651004](C:\Users\zgliang\AppData\Roaming\Typora\typora-user-images\image-20220718170651004.png)



softmax公式：

![image-20220718170726456](C:\Users\zgliang\AppData\Roaming\Typora\typora-user-images\image-20220718170726456.png)





![image-20220718171012845](C:\Users\zgliang\AppData\Roaming\Typora\typora-user-images\image-20220718171012845.png)



#### sigmoid（Logistic函数）

![image-20220718171310052](C:\Users\zgliang\AppData\Roaming\Typora\typora-user-images\image-20220718171310052.png)

sigmoid函数公式：

![image-20220718171358897](C:\Users\zgliang\AppData\Roaming\Typora\typora-user-images\image-20220718171358897.png)



### 常见的交叉熵损失函数类型

![image-20220718171522751](C:\Users\zgliang\AppData\Roaming\Typora\typora-user-images\image-20220718171522751.png)

#### 交叉熵损失函数详解（这个好像是错的）

弹幕：这里应该是softmax激活，然后交叉熵算出损失，下一步应该是BP优化吧？

![image-20220718171716178](C:\Users\zgliang\AppData\Roaming\Typora\typora-user-images\image-20220718171716178.png)

这个就是交叉熵最简公式的表达，注意这里的log是以e为底的。

举例计算如下：

![image-20220718172752006](C:\Users\zgliang\AppData\Roaming\Typora\typora-user-images\image-20220718172752006.png)

上图是softmax作为激活函数的交叉熵



![image-20221119172321150](C:\Users\zgliang\AppData\Roaming\Typora\typora-user-images\image-20221119172321150.png)

不过上图还是没看懂？？？？？？？？





## 交叉熵损失函数

[损失函数｜交叉熵损失函数 - 知乎 (zhihu.com)](https://zhuanlan.zhihu.com/p/35709485)



### softmax+交叉熵

在深度学习中，经常将交叉熵函数与 softmax 函数相关联，这是因为 softmax 函数对一组数据的每个数据进行计算后，可以将这组数据中的每一个都映射到 (0,1) 区间上，并且其和为 1，符合概率的定义，而交叉熵恰恰是对概率分布进行计算，因此通常将 softmax 函数的输出结果作为交叉熵函数的输入，将二者构造为复合函数。

在深度学习的训练过程中，通常将每个输入的真实标签值设置为分布 p，预测值设置为分布 q，利用交叉熵函数计算出的值作为损失值，将其用于后续的反向传播计算梯度并更新权重。在这里，设 softmax 函数对输入数据 ![x](https://math.jianshu.com/math?formula=x) 预测为第 ![i](https://math.jianshu.com/math?formula=i) 类的概率为 ![p_i](https://math.jianshu.com/math?formula=p_i)，输入数据 ![x](https://math.jianshu.com/math?formula=x) 属于第 ![i](https://math.jianshu.com/math?formula=i) 类的真实概率为 ![y_i](https://math.jianshu.com/math?formula=y_i)，那么交叉熵函数的形式为：

![H(y_i,p_i)=-\sum_{i}y_i\log p_i](https://math.jianshu.com/math?formula=H(y_i%2Cp_i)%3D-%5Csum_%7Bi%7Dy_i%5Clog%20p_i)

根据以上公式，对模型预测的第 ![i](https://math.jianshu.com/math?formula=i) 类的导数为：

![\frac{\partial H(y_i,p_i)}{\partial p_i}=-\frac{y_i}{p_i}](https://math.jianshu.com/math?formula=%5Cfrac%7B%5Cpartial%20H(y_i%2Cp_i)%7D%7B%5Cpartial%20p_i%7D%3D-%5Cfrac%7By_i%7D%7Bp_i%7D)

softmax 函数对输入的导数的形式可由 softmax 本身得到，其形式十分简单（由于自然对数 ![e](https://math.jianshu.com/math?formula=e) 的存在）：

![\frac{\partial softmax(x_i)}{\partial x_i}=softmax(x_i)(1-softmax(x_i))](https://math.jianshu.com/math?formula=%5Cfrac%7B%5Cpartial%20softmax(x_i)%7D%7B%5Cpartial%20x_i%7D%3Dsoftmax(x_i)(1-softmax(x_i)))

将 softmax 函数的输出作为交叉熵函数中的概率 ![p_i](https://math.jianshu.com/math?formula=p_i)，即 ![H(y_i,softmax(x_i))=-\sum_{i}y_i\log softmax(x_i)](https://math.jianshu.com/math?formula=H(y_i%2Csoftmax(x_i))%3D-%5Csum_%7Bi%7Dy_i%5Clog%20softmax(x_i))，那么交叉熵函数对 softmax 函数的输入![x_i](https://math.jianshu.com/math?formula=x_i)的导数为：

![\frac{\partial H(y_i,x_i)}{\partial x_i}=\frac{\partial H(y_i,p_i)}{\partial p_i}\frac{\partial p_i}{\partial x_i}](https://math.jianshu.com/math?formula=%5Cfrac%7B%5Cpartial%20H(y_i%2Cx_i)%7D%7B%5Cpartial%20x_i%7D%3D%5Cfrac%7B%5Cpartial%20H(y_i%2Cp_i)%7D%7B%5Cpartial%20p_i%7D%5Cfrac%7B%5Cpartial%20p_i%7D%7B%5Cpartial%20x_i%7D)

由于这里将 softmax 函数视作 p，即：

![softmax(x_i)=p_i](https://math.jianshu.com/math?formula=softmax(x_i)%3Dp_i)

将 softmax 用 p 代替，也就是将 ![softmax(x_i)](https://math.jianshu.com/math?formula=softmax(x_i)) 替换为 ![p_i](https://math.jianshu.com/math?formula=p_i)：

![\frac{\partial p_i}{\partial x_i}=p_i(1-p_i)](https://math.jianshu.com/math?formula=%5Cfrac%7B%5Cpartial%20p_i%7D%7B%5Cpartial%20x_i%7D%3Dp_i(1-p_i))

最后将上式代入到交叉熵函数的导数 ![{\frac{\partial H(y_i,x_i)}{\partial x_i}}](https://math.jianshu.com/math?formula=%7B%5Cfrac%7B%5Cpartial%20H(y_i%2Cx_i)%7D%7B%5Cpartial%20x_i%7D%7D) 中，即可得到交叉熵函数对输入 ![x_i](https://math.jianshu.com/math?formula=x_i) 的导数：

![{\frac{\partial H(y_i,x_i)}{\partial x_i}}={\frac{\partial H(y_i,p_i)}{\partial p_i}}{\frac{\partial p_i}{\partial x_i}}=-{\frac{y_i}{p_i}p_i(1-p_i)}=y_{i}(p_{i}-1)](https://math.jianshu.com/math?formula=%7B%5Cfrac%7B%5Cpartial%20H(y_i%2Cx_i)%7D%7B%5Cpartial%20x_i%7D%7D%3D%7B%5Cfrac%7B%5Cpartial%20H(y_i%2Cp_i)%7D%7B%5Cpartial%20p_i%7D%7D%7B%5Cfrac%7B%5Cpartial%20p_i%7D%7B%5Cpartial%20x_i%7D%7D%3D-%7B%5Cfrac%7By_i%7D%7Bp_i%7Dp_i(1-p_i)%7D%3Dy_%7Bi%7D(p_%7Bi%7D-1))

可以看到交叉熵函数与 softmax 函数结合使用，可以得到十分简洁的导数形式，只需将 softmax 的输出结果减 1 再与对应的标签值 ![y_i](https://math.jianshu.com/math?formula=y_i) 相乘即可得到在第 ![i](https://math.jianshu.com/math?formula=i) 类上的导数，对每个类别分别计算相应的导数，即可得到我们需要的梯度。在许多任务中，标签值往往用 one-hot 形式表示，![y_i](https://math.jianshu.com/math?formula=y_i) 一般为 1，那么只需将 softmax 函数的计算结果减 1 即可得到本次传播的第 ![i](https://math.jianshu.com/math?formula=i) 类的导数值，这使得反向传播中梯度的计算变得十分简单和方便。

### sigmoid+交叉熵损失函数

![image-20221120212901815](C:\Users\zgliang\AppData\Roaming\Typora\typora-user-images\image-20221120212901815.png)

![image-20221120212941292](C:\Users\zgliang\AppData\Roaming\Typora\typora-user-images\image-20221120212941292.png)

![image-20221120213001942](C:\Users\zgliang\AppData\Roaming\Typora\typora-user-images\image-20221120213001942.png)



## 极大似然估计、最小二乘法

这两个方法都是**定量**估计两个模型之间的差异，上面的交叉熵也是定量。





# 优化器/损失函数

![image-20221120184255616](C:\Users\zgliang\AppData\Roaming\Typora\typora-user-images\image-20221120184255616.png)

迭代k次之后（fstar是极值点），误差可以达到的量级。

## 牛顿法

![image-20221120194509700](C:\Users\zgliang\AppData\Roaming\Typora\typora-user-images\image-20221120194509700.png)

绿色的线是用的牛顿法（其实就是用这点的二阶泰勒近似），红色的线是用的在这一点的梯度，最下面灰色的线表示最佳路线。

注意这里是没有学习率η的，因为只有取的步长为计算出的长度时候，才可以到底二次曲线的顶点。

以上是只有一维变量的情况。

以下看更高维度的情况：

![image-20221120195701324](C:\Users\zgliang\AppData\Roaming\Typora\typora-user-images\image-20221120195701324.png)

H(W)是梯度的梯度，放在分母的位置上。

缺点是：每次都计算除这样的一个矩阵，计算量太大了。

## SGD

随机梯度下降（是随机挑选一个批次的数据来进行计算）也就是现在所说的mini-batch

![image-20221119173808448](C:\Users\zgliang\AppData\Roaming\Typora\typora-user-images\image-20221119173808448.png)



## SGD+Momentum

为了防止陷入局部最优解，加上了上一次梯度的方向作为参考

![image-20221119174023499](C:\Users\zgliang\AppData\Roaming\Typora\typora-user-images\image-20221119174023499.png)



## 动量法（参考过往数据）

又叫冲量法

![image-20221120200204318](C:\Users\zgliang\AppData\Roaming\Typora\typora-user-images\image-20221120200204318.png)

让历史的数据也考虑进来，对参数的修改进行了一些修正，让其变化不只是参考本点的梯度，变化不那么剧烈。



这种加上β的方法，叫做指数加权平均法。

![image-20221120200255607](C:\Users\zgliang\AppData\Roaming\Typora\typora-user-images\image-20221120200255607.png)

距离本次越远的，那么其指数占比越小。



## Nesterov（预知未来）

有的地方叫“牛顿冲量法”，跟前面的“冲量法”区分开。

![image-20221120202545229](C:\Users\zgliang\AppData\Roaming\Typora\typora-user-images\image-20221120202545229.png)

其实这里还是有些迷糊。。。（视频21min处）





## AdaGrad

自动调整learning_rate，因为learning_rate本来就不应该是固定的，因为learning_rate固定的话，很有可能在收敛点来回震荡，所以learning_rate应该越来越小。

![image-20221120203351687](C:\Users\zgliang\AppData\Roaming\Typora\typora-user-images\image-20221120203351687.png)

说明：是在学习率η的分母下面写上了“历史所有梯度数据的平方和再开方”，如果历史数据修改的多，证明梯度较大，那么学习率就下降的比较快，学习率就减少的比较多，欸布希隆是一个避免分母为0的极小量，随意取。

actually：不应该解释成先平方再开方，应该解释成梯度的内积开方，学习到的梯度是真实梯度除以梯度内积的开方。**adagrad本质是解决各方向导数数值量级的不一致而将梯度数值归一化。**

AdaGrad方法特别**适合稀疏数据**。如果一个分类的数据集的分类依赖的是他们特征不同，而不是在某个特征上的明显程度不同，那么这个数据集就是一个稀疏数据集。随着维度的增加，那么遇到稀疏数据集的可能性会越来越高。



## RMSProp

![image-20221120160325769](C:\Users\zgliang\AppData\Roaming\Typora\typora-user-images\image-20221120160325769.png)

## Adam

结合RMSProp和动量法。

感觉就是用RMSProp的思路揉了SGDMomentum的动量，同时考虑了上一步的梯度方向和学习率的更新



![image-20221120160348023](C:\Users\zgliang\AppData\Roaming\Typora\typora-user-images\image-20221120160348023.png)



### 

# 梯度爆炸 & 梯度消失

## 原因

梯度的计算方法：

![image-20221120210245510](C:\Users\zgliang\AppData\Roaming\Typora\typora-user-images\image-20221120210245510.png)



![image-20221120210324774](C:\Users\zgliang\AppData\Roaming\Typora\typora-user-images\image-20221120210324774.png)



## relu神经元的失活问题

![image-20221120210351599](C:\Users\zgliang\AppData\Roaming\Typora\typora-user-images\image-20221120210351599.png)





